{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 予習：ニューラルネットワークの数学的要素\n",
    "ここでは、キーワードっぽいものと大切な知識のみ記録していく."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 初めてのニューラルネットワーク\n",
    "- クラスとラベル\n",
    "\n",
    "> クラスとラベル\n",
    "> 機械学習では、分類問題の**カテゴリ**(category)を**クラス**(class)と呼び、データ転を**サンプル**(sample)または**標本**と呼びます。そして、特定のサンプルに関連づけられているクラスを**ラベル**(label)と言います。\n",
    "\n",
    "これをもとに具体例を見ていきましょう。\n",
    "- train_images, train_labels → 訓練データセット\n",
    "  - モデルが学習するデータ\n",
    "- test_images, test_labels →テストデータセット\n",
    "  - テストするデータ\n",
    "\n",
    "今MNISTの手書き数字のデータの分類問題をしていて、1つ1つの写真は 28x28ピクセルのグレースケール画像です。\n",
    "このとき、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 19:21:15.312468: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_lables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m train_images\u001b[39m.\u001b[39mshape \u001b[39m## → (60000, 28, 28)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mlen\u001b[39m(train_images) \u001b[39m## → 60000\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m train_lables \u001b[39m## → array([5,0,4, ... ,5,6,8], dtype=unit8)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m test_images\u001b[39m.\u001b[39mshape \u001b[39m## → (10000,28,28)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mlen\u001b[39m(test_images) \u001b[39m## →10000\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_lables' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images,train_labels),(test_images,test_labels) = mnist.load_data()\n",
    "\n",
    "train_images.shape ## → (60000, 28, 28)\n",
    "len(train_images) ## → 60000\n",
    "train_lables ## → array([5,0,4, ... ,5,6,8], dtype=unit8)\n",
    "\n",
    "test_images.shape ## → (10000,28,28)\n",
    "len(test_images) ## →10000\n",
    "test_lables ## → array([7,2,1, ... , 4,5,6], dtype=unit8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "です。\n",
    "\n",
    "\n",
    "ワークフローは以下のようです。\n",
    "1. ニューラルネットワーク（モデル）に訓練データ（`train_images`, `train_labels`）を供給する。\n",
    "2. ニューラルネットワークが画像トラベルの関連づけを学習する。\n",
    "3. テストデータ(`test_images`)での予測値をニューラルネットワークに生成させ、それらの予測値が`test_labels`と一致するかどうかを検証します。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ニューラルネットワークについて\n",
    "\n",
    "ニューラルネットワークの中核的な構成要素は**層**(layer = データのフィルタ)というデータ処理モジュールです。\n",
    "\n",
    "```python\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512,activation=\"relu\", input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10, activation=\"softmax\"))\n",
    "```\n",
    "\n",
    "具体的にやっていることは、**これらの層は入力されたデータから表現(repression)を抽出し、うまくいけば、現在取り組んでいる問題にとってより意味のある表現を抽出する**ことです。\n",
    "\n",
    "ディープラーニングはこれらの単純な層をつなぎ合わせたものとして構成されており、それらのそうは段階的なデータ蒸留(data distillation)を実装することになります。結局NNはさまざまな層を組み合わせて意味のあるデータを抽出しようとすることです。\n",
    "\n",
    "このニューラルネットワークは2つの連続する密に結合された（Dense）（＝蜜月号されたニューラル層、**全結合層(fully connected layer)**）で構成されています。\n",
    "\n",
    "2つ目の層は10個のユニットからなる**ソフトマックス(softmax)**層であり、合計すると1になる10個の確率スコアが含まれた配列を返しています。各スコアは現在の数字の画像が10個の数字クラスのいずれかに属している確率を表しています。\n",
    "\n",
    "ニューラルネットワークを訓練するためには**コンパイル**ステップの一部として以下の3つの要素を選択する必要があります。\n",
    "1. 損失関数\n",
    "    訓練データでのネットワークの性能をどのように評価するのか、そしてネットワークを正しい方向にどのように向かわせるのかを決める方法。\n",
    "2. オプティマイザ\n",
    "    与えられたデータと損失関数に基づいてネットワークが自身を更新するためのメカニズム。\n",
    "3. 訓練とテストを監視するための指標\n",
    "    ここでは、正解率（画像が正しく分類された割合）のみを考慮する。\n",
    "\n",
    "```python\n",
    "network.compile(optimizer = \"rmsprop\", loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "```\n",
    "\n",
    "訓練する前に、\n",
    "データの前処理をする必要があります。つまり、データの形状をネットワークが期待するものに変更して、すべての値を[0,1]の間に収まるようにスケーリングする必要があリマス。\n",
    "\n",
    "例えば、今訓練データとして使用している画像は、\n",
    "> 型が`unit8`、形状が`(60000,28,28)`の配列に格納されています。この配列には`[0,255]`の区間の値が含まれている。\n",
    "> このデータを変換して\n",
    "> 型が`float32`、形状が`(60000,28*28)`で`0~1`の値が含まれているような形に変更する。\n",
    "\n",
    "``` python\n",
    "train_images = train_images.reshape((60000, 28*28)) # reshapeしてあげた\n",
    "train_images = train_images.astype(\"float32\")/255 # ここで値を0~1にした\n",
    "test_images = test_images.reshape((10000, 28*28)) # reshapeしてあげた\n",
    "test_images = test_images.astype(\"float32\")/255 # ここで値を0~1にした\n",
    "```\n",
    "\n",
    "またラベルもカテゴリの値でエンコードする必要があります。\n",
    "```python\n",
    "from keras.utilis\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 19:22:24.646860: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512,activation=\"relu\", input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体的にやっていることは、**これらの層は入力されたデータから表現(repression)を抽出し、うまくいけば、現在取り組んでいる問題にとってより意味のある表現を抽出する**ことです。\n",
    "\n",
    "ディープラーニングはこれらの単純な層をつなぎ合わせたものとして構成されており、それらのそうは段階的なデータ蒸留(data distillation)を実装することになります。結局NNはさまざまな層を組み合わせて意味のあるデータを抽出しようとすることです。\n",
    "\n",
    "このニューラルネットワークは2つの連続する密に結合された（Dense）（＝蜜月号されたニューラル層、**全結合層(fully connected layer)**）で構成されています。\n",
    "\n",
    "2つ目の層は10個のユニットからなる**ソフトマックス(softmax)**層であり、合計すると1になる10個の確率スコアが含まれた配列を返しています。各スコアは現在の数字の画像が10個の数字クラスのいずれかに属している確率を表しています。\n",
    "\n",
    "ニューラルネットワークを訓練するためには**コンパイル**ステップの一部として以下の3つの要素を選択する必要があります。\n",
    "1. 損失関数\n",
    "    訓練データでのネットワークの性能をどのように評価するのか、そしてネットワークを正しい方向にどのように向かわせるのかを決める方法。\n",
    "2. オプティマイザ\n",
    "    与えられたデータと損失関数に基づいてネットワークが自身を更新するためのメカニズム。\n",
    "3. 訓練とテストを監視するための指標\n",
    "    ここでは、正解率（画像が正しく分類された割合）のみを考慮する。\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "network.compile(optimizer = \"rmsprop\", loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練する前に、\n",
    "データの前処理をする必要があります。つまり、データの形状をネットワークが期待するものに変更して、すべての値を[0,1]の間に収まるようにスケーリングする必要があリマス。\n",
    "\n",
    "例えば、今訓練データとして使用している画像は、\n",
    "> 型が`unit8`、形状が`(60000,28,28)`の配列に格納されています。この配列には`[0,255]`の区間の値が含まれている。\n",
    "> このデータを変換して\n",
    "> 型が`float32`、形状が`(60000,28*28)`で`0~1`の値が含まれているような形に変更する。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28*28)) # reshapeしてあげた\n",
    "train_images = train_images.astype(\"float32\")/255 # ここで値を0~1にした\n",
    "test_images = test_images.reshape((10000, 28*28)) # reshapeしてあげた\n",
    "test_images = test_images.astype(\"float32\")/255 # ここで値を0~1にした"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "またラベルもカテゴリの値でエンコードする必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.utilis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilis\u001b[39;00m \u001b[39mimport\u001b[39;00m to_categorical\n\u001b[1;32m      3\u001b[0m train_labels \u001b[39m=\u001b[39m to_categorical(train_labels)\n\u001b[1;32m      4\u001b[0m test_labels \u001b[39m=\u001b[39m to_categorical(test_labels)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.utilis'"
     ]
    }
   ],
   "source": [
    "from keras.utilis import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでネットワークの訓練をすることができマス。Kerasでネットワークを訓練するにはネットワークの`fit`メソッドを呼び出して、モデルを訓練データに`適合`(fit)させる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m network\u001b[39m.\u001b[39;49mfit(train_images,train_labels,epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, batch_size \u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/engine/training.py:3618\u001b[0m, in \u001b[0;36mModel._assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_compile_was_called\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   3613\u001b[0m     \u001b[39m# Checks whether `compile` has been called. If it has been called,\u001b[39;00m\n\u001b[1;32m   3614\u001b[0m     \u001b[39m# then the optimizer is set. This is different from whether the\u001b[39;00m\n\u001b[1;32m   3615\u001b[0m     \u001b[39m# model is compiled\u001b[39;00m\n\u001b[1;32m   3616\u001b[0m     \u001b[39m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[39;00m\n\u001b[1;32m   3617\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_compiled:\n\u001b[0;32m-> 3618\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   3619\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou must compile your model before \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3620\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mtraining/testing. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3621\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUse `model.compile(optimizer, loss)`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3622\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "network.fit(train_images,train_labels,epochs=5, batch_size =128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これをすると、\n",
    "> train_data → 98.9% test_data → 97.8%\n",
    "\n",
    "が出る。つまり、**過学習(overfitting)** の一例になっている。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 ニューラルネトワークでのデータ表現\n",
    "以上の例では、多次元のNumpy配列に格納されたデータを扱いました。これらの配列を**テンソル(tensor)**と言います。テンソルの **次元** を**軸（axis）** という。基本的にはテンソルはデータのコンテナ（入れ物）です。\n",
    "\n",
    "### 2.2.1 スカラー\n",
    "スカラーは0次元テンソルです。　Numpyのfloar32型やfloat64型はスカラーテンソルです。\n",
    "\n",
    "Numpyでテンソルの軸の数を表示するには`ndim`属性を使用します。スカラーテンソルの軸の数は0です(ndim == 0)。テンソルの軸の数は**階数(rank)** と言います。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is 12\n",
      "x.ndim is 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array(12)\n",
    "print(\"x is\" ,x)\n",
    "print(\"x.ndim is\", x.ndim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2　ベクトル：1次元テンソル\n",
    "数値の配列は**ベクトル(vector)**と呼ばれます。ベクトルは1次元テンソルです。1次元テンソルの軸は1つです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is [12  3  5 15 17]\n",
      "x.ndim is 1\n"
     ]
    }
   ],
   "source": [
    "x = np.array([12,3,5,15,17])\n",
    "print(\"x is\", x)\n",
    "print(\"x.ndim is\", x.ndim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このベクトルの要素数は5なので**5次元ベクトル**です。5次元ベクトルと5次元テンソルは違いますよ。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 行例：2次元テンソル\n",
    "ベクトルの配列は**行列(matrix)**です。行列は行と列の2つの軸を持つので2次元テンソルです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is [[ 5 78  3 45  0]\n",
      " [ 6  2  3  4  1]\n",
      " [ 7 35 34 45  2]]\n",
      "x.ndim is 2\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[5,78,3,45,0],[6,2,3,4,1],[7,35,34,45,2]])\n",
    "print(\"x is\", x)\n",
    "print(\"x.ndim is\", x.ndim)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 3次元テンソルとそれよりも高次元のテンソル\n",
    "そうした行列を新しい配列に詰め込むと、3次元テンソルになります。めんどくさいので具体例は省略します。ディープラーニングでは大体4次元テンソルで、動画を扱うときは5次元のテンソルを扱います。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 テンソルの重要な属性\n",
    "- **軸の数(階数)**\n",
    "\n",
    "\n",
    "    例えば3次元テンソルの軸は3つで、行列の軸は2つです。NumPyなどのPythonライブラリでは軸の数をテンソルのndim属性とも呼びます。\n",
    "- **形状**\n",
    "\n",
    "\n",
    "    テンソルの各軸に沿った次元の数を表す整数のタプル。例えば先の例では、行列の形状は(3,5)です。ベクトルの形状は（5,）のように単一の要素で表されますがスカラーの形状は空(())になります。\n",
    "- **データの型**\n",
    "\n",
    "\n",
    "    テンソルに含まれているデータの型。Pythonライブラリでは、通常はdtypeで表されます。例えば、テンソルの型はfloat32, unit8, float64などになります。なお、まれにchar型のテンソルが使用されることもあります。NumPyをはじめとするほとんどのライブラリでは、文字列型テンソルは存在しません。というのもテンソルはあらかじめ確保された連続するメモリ領域に存在しますが、文字列は可変調であり、そうした実装は不可能だからです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(60000, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "print(train_images.ndim) # 3\n",
    "\n",
    "print(train_images.shape) # (60000,28,28)\n",
    "\n",
    "print(train_images.dtype) # unit8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これからわかるように、train_imagesは28x28個の整数からなる60000この行列が含まれた配列です。それらの行列はそれぞれ0~255の係数を持つグレースケール画像です。4つ目のimageを描画してみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digit = train_images[4]\n",
    "# print(digit)\n",
    "print(digit.ndim)\n",
    "print(digit.shape) # (28,28)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(digit, cmap =plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6 Numpyでのテンソルの操作\n",
    "テンソルの特定の要素を選択することを **テンソル分解(tensor slicing)** と呼びます。\n",
    "\n",
    "次の例では10番目から100番目の手前までの数字を選択してそれらを(90,28,28)という形状の配列に配置します。\n",
    "\n",
    "この時負の番号を引数に入れたら、軸の終端を基準とした相対位置を表します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "my_slice = train_images[10:100]\n",
    "print(my_slice.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.7 データバッチ\n",
    "一般に、DLで使用されるデータテンソルの最初の軸は**サンプル軸(sample axis)(= サンプル次元)**と言います。それに加えて、DLのモデルでは、データセット全体を一度に処理するのではなく、データを小さなバッチに分割します。具体的には、このMNISTのデータセットのバッチサイズは128なので、1つのバッチは次のように定義される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_images[:128] # train images[:128,:,:]と同じようなテンソル分解をしている"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2つめのバッチは以下のよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_images[128:256]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n番目のバッチは以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barch = train_images[128*(n-1):128*n]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようなバッチテンソルを考える時、最初の軸は**バッチ軸(= バッチ次元)** と呼びます。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.8 データテンソルの現実的な例\n",
    "具体的に扱うのは以下のようなデータです。\n",
    "- ベクトルデータ\n",
    "\n",
    "\n",
    "    形状が(samples, features)の２次元テンソル\n",
    "- 時系列データまたはシーケンス（系列）データ\n",
    "\n",
    "\n",
    "    形状が(samples, timesteps,features)の３次元テンソル\n",
    "- 画像\n",
    "\n",
    "\n",
    "    形状が(samples, height, width, channels)または(samples, channels, height, width)の４次元テンソル\n",
    "- 動画\n",
    "\n",
    "\n",
    "    形状が(samples,frames, height, width, channels)または(samples,frames, channels, height, width)の５次元テンソル"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.9 ベクトルデータ\n",
    "データ点をそれぞれベクトルとしてエンコードすることができる。このため、データバッチは２次元テンソル(ベクトルの配列)としてエンコードされます。この場合1つ目の軸が**サンプル軸**であり、2つ目の軸が**特徴軸**です。\n",
    "\n",
    "例)\n",
    "\n",
    "1. 人の年齢、郵便番号、住所と収入をまとめた生命表データセット\n",
    "2. テキスト文書からなるデータセット\n",
    "\n",
    "### 2.2.10 時系列データとシーケンスデータ\n",
    "データにおいて時間（またはシーケンスの順序）が重要となる場合は、常に、そうしたデータを明示的な時間軸を持つ3次元テンソルに格納するのが理にかなっている。\n",
    "\n",
    "時間軸は常に2つ目の軸にするのが慣例になっている。\n",
    "\n",
    "例)\n",
    "\n",
    "1. 株価のデータセット\n",
    "2. ツイートのデータセット\n",
    "\n",
    "### 2.2.11 画像データ\n",
    "一般に画像は、幅、高さ、色深度の3つのチャネルで表されます。画像テンソルは常に3次元であり、ゲレースケールならば1次元のカラーチャネルを使用する。したがって、サンプルサイズが256 x 256のグレースケール画像が128個含まれたバッチは、形状が(128,256,256,1)のテンソルに格納できます。\n",
    "\n",
    "TensorFlowは**チャネルラスト**の表現方法に則り(samples,height,width,channels)と配置します。\n",
    "\n",
    "### 2.2.12 動画データ\n",
    "5次元テンソルが必要なのは限られているけど動画とか。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 ニューラルネットワークの歯車：テンソル演算\n",
    "DLによって学習された変換は全て数値データのテンソルに適用される一握りの**テンソル演算** に分解できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 要素ごとの演算\n",
    "大変そうだけど、pythonだとベクトルかの実装に優れてるよ、っていうお話。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 ブロードキャスト\n",
    "2つのテンソルの形状が異なるとき、**小さい方が大きい方のテンソルに形状を合わせてブロードキャストされる** 。ただし、前提として可能であることと曖昧さがないことが条件になります。\n",
    "\n",
    "手順は以下のよう。\n",
    "\n",
    "1. ブロードキャスト軸(broadcast axis) を小さい方のテンソルに追加することで、大きいテンソルと次元(ndim)が同じになるようにする\n",
    "2. 小さい方のテンソルを新しい軸上で繰り返すことで、大きいテンソルと完全に同じ形状にする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 3, 32, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.random((64,3,32,10))\n",
    "y = np.random.random((32,10))\n",
    "\n",
    "z = np.maximum(x,y)\n",
    "print(z.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 テンソルの内積\n",
    "普通の行列の席と同じですな。\n",
    "\n",
    "`np.dot(x,y)`と`x*y`の違いに気をつけましょう。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 テンソルの変形\n",
    "**テンソルの変形(tensor reshaping)** は、目的の形状に一致するようにテンソルの行と列の配置を変更することを意味する。もちろん、**変形したテンソルの要素数と変形後のテンソルの要素数の総数は同じ** になる必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [2. 3.]\n",
      " [4. 5.]]\n",
      "(3, 2)\n",
      "[[0. 1. 2.]\n",
      " [3. 4. 5.]]\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.,1.],[2.,3.],[4.,5.]])\n",
    "print(x)\n",
    "print(x.shape) # (3,2)\n",
    "\n",
    "x = x.reshape((6,1))\n",
    "# print(x)\n",
    "# print(x.shape) # (6,1)\n",
    "\n",
    "x = x.reshape((2,3))\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**転置(transposition)** を実装してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 300)\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((300,20))\n",
    "x = x.T # np.transpose(x)でもOK\n",
    "print(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 テンソル演算の幾何学的解釈\n",
    "これは大学受験でやった\n",
    "\n",
    "### 2.3.6 DLの幾何学的解釈\n",
    "DLのアプローチは、複雑な幾何学変換を基本的な変換の連鎖として少しずつ分解していくことと同義なのだ！（わー）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 NNのエンジン：勾配ベースの最適化\n",
    "まずReLUについて\n",
    "```python\n",
    "output = relu(dot(W,input) + b)\n",
    "```\n",
    "で、Wとbはテンソルでそれらは層の属性です。重みと呼びますよ〜。これらの重み行列には、ネットワークが訓練データから学習した情報が含まれています。\n",
    "\n",
    "これらの重み行列には、初期値として小さな乱数値が設定されていて、これを**ランダム初期化(random initialization)** と言います。出発点ではこれらの重みは無意味であるが、フィードバックを用いて少しずつ調整されていきます。この調整を**訓練**といい、機械学習の目標である学習に匹敵するのです。\n",
    "\n",
    "また、この学習は**訓練ループ** の中で行われて以下のような仕組みになっています。\n",
    "\n",
    "1. 訓練データ`x`と対応する目的値`y`をバッチデータとして抽出する\n",
    "2. ネットワークをxで実行して、予測値`y_pred`を取得する（= **forward propagation**）\n",
    "3. このバッチでの損失値を計算する。これは、`y_pred`と`y`との不一致の目明日となる指標です。\n",
    "4. このバッチでの損失値が少し小さくなるようにネットワークの全ての重みを更新する。\n",
    "\n",
    "最終的に損失値が非常に小さいネットワークが得られる（＝学習済み）。\n",
    "\n",
    "この時に重みを更新するときに微分を使います。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1　導関数\n",
    "省略\n",
    "### 2.4.2 テンソル演算の導関数：勾配\n",
    "省略\n",
    "### 2.4.3 確率的勾配降下法\n",
    "微分可能だったら、理論的には最小値を求めることはできます、が実際には計算が煩雑でめっちゃ大変です。\n",
    "\n",
    "それに変わる方法として、上述のアルゴリズムを使う方法があります。つまり、\n",
    "> ランダムなデータバッチでの現在の損失値に基づいて、パラメータを少しずつ調整していく\n",
    "\n",
    "のです。\n",
    "\n",
    "1. 訓練データ`x`と対応する目的値`y`をバッチデータとして抽出する\n",
    "2. ネットワークをxで実行して、予測値`y_pred`を取得する（= **forward propagation**）\n",
    "3. このバッチでの損失値を計算する。これは、`y_pred`と`y`との不一致の目明日となる指標です。\n",
    "4. ネットワークのパラメータを調整するために損失関数の勾配を計算する。この手順は**バックワードパス(backward pass)** と呼ばれる。\n",
    "5. 例えば $W -= \\mathrm{step} \\times \\mathrm{gradient}$を実行することで、そのパラメータを勾配とは逆方向に少し移動させることでこのバッチでの損失値を少し小さくする\n",
    "\n",
    "これをする。これは、**ミニバッチ確率的勾配降下法(mini-batch stochastic gradient descent)** と呼びますよ。「確率的」とは、各データバッチがランダムに抽出されることに由来する。要するに「ランダム」ってことです。\n",
    "\n",
    "このとき、**step係数を適切に選択することが大変重要です** 。\n",
    "\n",
    "ミニバッチ確率的勾配降下法の1つに、データバッチを抽出するのではなく、イテレーションごとにサンプルと目的値を1つだけ抽出するものがある。これは（ミニバッチではなく）真の確率的勾配法といいます。あるいは、逆に利用可能な全てのデータで全ての手順を実行することもできる。これを**バッチ確率的勾配法**と言います。両方ともめっちゃコストかかります。\n",
    "\n",
    "また、ミニバッチ確率的勾配降下法(SGD)には、次の重みの更新を計算するときに、単に購買の現在の値を調べるのではなく、以前の重みの更新を考慮に入れるものが何種類かあります。例えばモーメンタムSGD、AdaGrad, RMSPropなどがあります。これらのSGDは**最適化法(optimization method)** や**オプティマイザ（optimizer）** と呼ばれる。\n",
    "\n",
    "モーメンタムを使うと、極小値に止まるだけではなく、大域的最小値にまで辿り着くことができます。\n",
    "\n",
    "\n",
    "### 2.4.4 導関数の連鎖：バックプロパゲーションアルゴリズム\n",
    "\n",
    "NNの関数は、それぞれ単純な既知の微分を持つ多くのテンソル演算をつなぎ合わせたものでできています。例えば3つのテンソル演算a, b, cと重み行列W1, W2, W3からなるネットワークfは次のように定義されます。\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathrm{f}(W1, W2, W3) = \\mathrm{a}(W1,\\mathrm{b}(W2,\\mathrm{c}(W3)))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "こうした関数の連鎖は**連鎖律(chain rules)** と呼ばれる恒等式を使って微分することができます。\n",
    "\n",
    "NNの勾配値の計算に連鎖率を適応すると、**バックプロパゲーション** と呼ばれるアルゴリズムが得られます。これは、最終的な損失値を出発点として、出力側の層から入力側の層に向かって逆方向に進みます。そして連鎖率を適用することで各パラメータがその損失値にどのような影響を与えたのかを計算します。\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 最初の例を振り返る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/engine/training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 10) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m network\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrmsprop\u001b[39m\u001b[39m\"\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     21\u001b[0m \u001b[39m# 訓練ループの設定\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m network\u001b[39m.\u001b[39;49mfit(train_images, train_labels, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/wh/8g84my_j6wxc7w6pms6k18ww0000gn/T/__autograph_generated_filejkjmkfwv.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/engine/training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/zenonrk/opt/anaconda3/envs/deeplearning/lib/python3.9/site-packages/keras/backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 10) are incompatible\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images,train_labels),(test_images,test_labels) = mnist.load_data()\n",
    "\n",
    "# データの前処理\n",
    "# print(train_images.shape) # (60000,28,28)\n",
    "train_images = train_images.reshape((60000,28 * 28))\n",
    "# print(train_images.shape) # (60000,784)\n",
    "\n",
    "train_images = train_images.astype(\"float32\")/255  # 全ての値を[0,1]に収まるようにスケーリングした。もともと型がuint8で[0,255]の区間の値が含まれている\n",
    "test_images = test_images.reshape((10000, 28*28))\n",
    "test_images = test_images.astype(\"float32\")/255  \n",
    "\n",
    "# ネットワークの構築\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512,activation=\"relu\", input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# ネットワークのコンパイル\n",
    "network.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# 訓練ループの設定\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4683b78d7ec118a5736f9f91dc1c4b4f0fb8ece4c9a869fc1b20e6bb285f951"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
